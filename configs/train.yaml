experiment: 'train'
seed: 20
train: True

# dataset, model, hyperparameter, 학습방법, index
# dataset
train_dataset: [
  '/drl_nas1/ckddls1321/dev/DL/AudioRAG/data/json_files/AudioSet/train.json',
  '/drl_nas1/ckddls1321/dev/DL/AudioRAG/data/json_files/Clotho/train.json',
]
val_dataset: [
  "/drl_nas1/ckddls1321/dev/DL/AudioRAG/data/json_files/Clotho/val.json",
]
filtering: ""

# data args
data_args:
  dataset: "Clotho"
  global_batch_size: 256
  batch_size: 4
  num_workers: 4
  shuffle: False

# audio args
audio_args:
  sr: 48000
  n_fft: 1024
  hop_length: 320
  f_min: 50
  f_max: 14000
  n_mels: 64
  max_length: 10
  mono: True

# model args: encoder, align, peft
model_args:
  name: "CLAP2LLAMA"
  retr_prompt: ""
  task_prompt: "describe audio briefly"
  freeze_am: True
  unfreeze_am: [] # 특정 layer만 unfreeze
  freeze_lm: True # peft나 finetuning을 하면 false로 둔다.
  checkpoint_path: "./pretrained_models/pretrained_MLP/"
  device: "cuda:0"
  encoder:
    model_name: "CLAPAudioEncoder"
    pretrained: True
    freeze: True
    # fine grained, embedding, projected
    select_feature: "embedding"
    sequence_length: 1024
    hidden_size: 768
    window_size: 32
    step_size: 16
    device: "cuda:0"
  align:
    model_name: "MLP"
  peft_config:
    peft_type: "IA3"
    task_type: "CAUSAL_LM"
    target_modules: ["q_proj", "v_proj", "down_proj"] # k?
    feedforward_modules: ["down_proj"]
  decoder:
    sequence_max_length: 256

# hyperparameter
optim_args:
  lr: 1e-4
  # 뭐지? label smoothing?
  betas: [0.9, 0.99]
  eps: 1e-8
  # 뭐지?
  momentum: 0.9
  gamma: 0.1
  weight_decay: 0.01

# 학습방법
training:
  warmup_epochs: 1
  epochs: 10
  clip_grad: 2
  output_path: "./finetuned_models/finetuned_MLP/"
  eval: False
  validate: False

retrieval:
  index_path: "/data1/sungjun/data/final_index_big_kb_audiocaps/audio2audio_train.json"
  top_k: 2